import torch
from transformers import AutoProcessor, Qwen2VLForConditionalGeneration


class LLMService:
    def __init__(self, model_path=None, device=None):
        """
        åˆå§‹åŒ– LLMService

        model_path: æ¨¡å‹è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨ Qwen2-VL-2B-Instruct
        device: ä½¿ç”¨è®¾å¤‡ï¼Œé»˜è®¤è‡ªåŠ¨é€‰æ‹© CUDA æˆ– CPU
        """
        self.qwen_model_path = model_path or "/mnt/data/modelscope_cache/hub/Qwen/Qwen2-VL-2B-Instruct"
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        print(f"ğŸ§  æ­£åœ¨åŠ è½½ Qwen2-VL æ¨¡å‹ï¼š{self.qwen_model_path} åˆ°è®¾å¤‡ï¼š{self.device}")

        try:
            # åŠ è½½å¤šæ¨¡æ€å¤„ç†å™¨
            self.processor = AutoProcessor.from_pretrained(self.qwen_model_path)

            # åŠ è½½æ¨¡å‹
            self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                self.qwen_model_path,
                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
            ).eval()

            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")

        except Exception as e:
            raise RuntimeError(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")

    def generate_text(self, prompt: str, image=None, max_new_tokens=256):
        """
        ç”Ÿæˆæ–‡æœ¬

        prompt: æ–‡æœ¬è¾“å…¥
        image: å¯é€‰ï¼Œä¼ å…¥å›¾åƒï¼ˆPIL.Image æˆ– numpyï¼‰
        max_new_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦
        """
        try:
            # æ„é€ è¾“å…¥
            if image is not None:
                inputs = self.processor(
                    text=prompt,
                    images=image,
                    return_tensors="pt"
                ).to(self.device)
            else:
                inputs = self.processor(
                    text=prompt,
                    return_tensors="pt"
                ).to(self.device)

            # æ¨ç†ç”Ÿæˆ
            output = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens
            )

            # è§£ç è¾“å‡º
            result = self.processor.batch_decode(output, skip_special_tokens=True)[0]
            return result.strip()

        except Exception as e:
            return f"âš ï¸ ç”Ÿæˆæ–‡æœ¬æ—¶å‡ºé”™: {e}"

    def chat(self, prompt: str, image=None, max_new_tokens=256):
        """
        chat æ–¹æ³•ï¼Œå°è£… generate_text
        """
        return self.generate_text(prompt, image=image, max_new_tokens=max_new_tokens)


# ç‹¬ç«‹æµ‹è¯•è„šæœ¬ä½¿ç”¨
if __name__ == "__main__":
    model_path = "/home/jiang/.cache/modelscope/hub/Qwen/Qwen2-VL-2B-Instruct"
    llm = LLMService(model_path)

    # çº¯æ–‡æœ¬æµ‹è¯•
    prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
    response = llm.chat(prompt)
    print("ğŸ—£ï¸ æ¨¡å‹è¾“å‡ºï¼š", response)

    # å›¾æ–‡æµ‹è¯•ï¼ˆå¦‚æœæœ‰ PIL.Image å¯ä»¥ä¼ å…¥ï¼‰
    # from PIL import Image
    # img = Image.open("test.jpg")
    # response_img = llm.chat("è¯·æè¿°è¿™å¼ å›¾ç‰‡ã€‚", image=img)
    # print("ğŸ–¼ï¸ æ¨¡å‹è¾“å‡ºï¼š", response_img)




