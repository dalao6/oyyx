import os
import uuid
import logging
import pyaudio
import numpy as np
import soundfile as sf
import sherpa_onnx
import uvicorn
from fastapi import FastAPI, File, UploadFile
from pathlib import Path
from threading import Thread
import time
import noisereduce as nr
import webrtcvad
import socket
from collections import deque

# =========================
# æ—¥å¿—é…ç½®
# =========================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)

# =========================
# æ¨¡å‹è·¯å¾„é…ç½®
# =========================
ASR_MODEL_PATH = "/mnt/data/modelscope_cache/hub/xiaowangge/sherpa-onnx-sense-voice-small"

# =========================
# ä¸´æ—¶éŸ³é¢‘å­˜æ”¾ç›®å½•
# =========================
TMP_AUDIO_DIR = Path("./tmp_audio")
TMP_AUDIO_DIR.mkdir(exist_ok=True)

# =========================
# ASR æ¨¡å‹ç±»
# =========================
class STTModel:
    """ç¦»çº¿è¯­éŸ³è¯†åˆ«æ¨¡å‹"""
    def __init__(self, model_path: str, sample_rate: int = 16000, num_threads: int = 6):
        self.model_path = model_path
        self.sample_rate = sample_rate
        self.num_threads = num_threads
        self.recognizer = None

    def load_model(self):
        model_file = os.path.join(self.model_path, "model.onnx")
        token_file = os.path.join(self.model_path, "tokens.txt")
        if not os.path.exists(model_file) or not os.path.exists(token_file):
            raise FileNotFoundError("æ¨¡å‹æ–‡ä»¶æˆ– tokens.txt ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„")
        self.recognizer = sherpa_onnx.OfflineRecognizer.from_sense_voice(
            model=model_file,
            tokens=token_file,
            num_threads=self.num_threads,
            sample_rate=self.sample_rate,
            use_itn=True,
            language="auto",
            provider="cpu"
        )
        logger.info(f"âœ… ASR model loaded: {model_file}")

    def transcribe(self, audio_data: np.ndarray) -> str:
        if self.recognizer is None:
            raise RuntimeError("Model not loaded")
        stream = self.recognizer.create_stream()
        stream.accept_waveform(self.sample_rate, audio_data)
        self.recognizer.decode_stream(stream)
        return stream.result.text

# =========================
# FastAPI åˆå§‹åŒ–
# =========================
app = FastAPI(title="Offline ASR Service")
stt_model = STTModel(ASR_MODEL_PATH)
stt_model.load_model()

@app.post("/v1/stt")
async def speech_to_text(file: UploadFile = File(...)):
    tmp_file_path = TMP_AUDIO_DIR / f"{uuid.uuid4()}{Path(file.filename).suffix}"
    try:
        with open(tmp_file_path, "wb") as f:
            f.write(await file.read())
        audio_data, sr = sf.read(tmp_file_path, dtype='int16')
        text = stt_model.transcribe(audio_data)
        return {"code": 200, "msg": "success", "data": {"text": text}}
    except Exception as e:
        logger.error(f"STT error: {e}")
        return {"code": 500, "msg": str(e), "data": None}
    finally:
        if tmp_file_path.exists():
            tmp_file_path.unlink()

# =========================
# VAD + é™å™ª + å®æ—¶å½•éŸ³
# =========================
RATE = 16000
CHUNK = 1024
FRAME_MS = 20
FRAME_SIZE = int(RATE * FRAME_MS / 1000)
VAD_AGGRESSIVENESS = 2
vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)

# è¯­éŸ³æ£€æµ‹å‚æ•°
SPEECH_WINDOW = deque(maxlen=50)  # æ£€æµ‹çª—å£ï¼ˆ1ç§’å·¦å³ï¼‰
SPEECH_THRESHOLD = 0.6            # è¶…è¿‡å¤šå°‘æ¯”ä¾‹è®¤ä¸ºæ˜¯è¯­éŸ³
MAX_SILENCE_FRAMES = 50           # è¿ç»­é™éŸ³å¤šå°‘å¸§åè®¤ä¸ºè¯´è¯ç»“æŸ

def is_speech(frame_bytes):
    return vad.is_speech(frame_bytes, RATE)

def record_audio():
    p = pyaudio.PyAudio()
    try:
        stream = p.open(format=pyaudio.paInt16,
                        channels=1,
                        rate=RATE,
                        input=True,
                        frames_per_buffer=CHUNK)
    except Exception as e:
        logger.error(f"âŒ æ— æ³•æ‰“å¼€éº¦å…‹é£: {e}")
        return

    logger.info("ğŸ™ï¸ å¼€å§‹å½•éŸ³ (Ctrl+C åœæ­¢)...")

    frames = []
    silence_counter = 0
    in_speech = False

    try:
        while True:
            data = stream.read(CHUNK, exception_on_overflow=False)
            np_data = np.frombuffer(data, dtype=np.int16)

            # åˆ†å¸§æ£€æµ‹
            for i in range(0, len(np_data), FRAME_SIZE):
                frame = np_data[i:i + FRAME_SIZE]
                if len(frame) < FRAME_SIZE:
                    continue
                frame_bytes = frame.tobytes()
                speech_flag = is_speech(frame_bytes)
                SPEECH_WINDOW.append(1 if speech_flag else 0)

                if np.mean(SPEECH_WINDOW) > SPEECH_THRESHOLD:
                    if not in_speech:
                        logger.info("ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³å¼€å§‹")
                        in_speech = True
                        frames = []
                    frames.append(frame)
                    silence_counter = 0
                elif in_speech:
                    silence_counter += 1
                    frames.append(frame)
                    if silence_counter > MAX_SILENCE_FRAMES:
                        logger.info("ğŸ›‘ æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸï¼Œå¼€å§‹è¯†åˆ«...")
                        audio_chunk = np.concatenate(frames)

                        # é™å™ª
                        enhanced = nr.reduce_noise(y=audio_chunk.astype(np.float32), sr=RATE)
                        enhanced = enhanced.astype(np.int16)

                        # ASR
                        try:
                            text = stt_model.transcribe(enhanced)
                            if text.strip():
                                logger.info(f"ğŸ§  è¯†åˆ«ç»“æœ: {text}")
                            else:
                                logger.info("âš ï¸ æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³")
                        except Exception as e:
                            logger.error(f"ASR error: {e}")

                        in_speech = False
                        silence_counter = 0
                        frames = []
            time.sleep(0.01)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ åœæ­¢å½•éŸ³")
    finally:
        stream.stop_stream()
        stream.close()
        p.terminate()

# =========================
# è‡ªåŠ¨ç«¯å£é€‰æ‹©
# =========================
def get_free_port(default=7998):
    s = socket.socket()
    try:
        s.bind(("", default))
        port = s.getsockname()[1]
        return port
    except OSError:
        s.bind(("", 0))
        return s.getsockname()[1]
    finally:
        s.close()

def start_api():
    port = get_free_port()
    logger.info(f"ğŸš€ FastAPI æœåŠ¡å¯åŠ¨: http://0.0.0.0:{port}")
    uvicorn.run(app, host="0.0.0.0", port=port)

# =========================
# ä¸»ç¨‹åºå…¥å£
# =========================
if __name__ == "__main__":
    Thread(target=record_audio, daemon=True).start()
    start_api()
